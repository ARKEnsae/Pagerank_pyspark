{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendant les test pour stopper un SparkContext\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au cluster / test en local\n",
    "sc = SparkContext(\"local\", \"Page Rank With PySpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "# Préparation du fichier avant utilisation dans l'algo\n",
    "# Taille du fichier initial = dfFile.shape = (5105039, 2)\n",
    "#dfFile = pd.read_csv(\"web-Google.txt\", sep='\\t', header=3)\n",
    "# Pour les tests récupérations des 100 premières lignes uniquement \n",
    "dfFile = pd.read_csv(\"web-Google.txt\", sep='\\t', header=3, nrows = 20)\n",
    "#dfFile.rename(columns={'# FromNodeId':'FromNodeId'}, inplace = True)\n",
    "print(dfFile.shape)\n",
    "#dfFile.to_csv(\"web-GoogleSmall.txt\", sep='\\t', header=False, index=False)\n",
    "dfFile.to_csv(\"web-GoogleSmall.txt\", sep=' ', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file \"GoogleReduc.txt\" from a local file system (available on all nodes),\n",
    "# and return it as an RDD of Strings.\n",
    "RddDataBase = sc.textFile(\"web-GoogleSmall.txt\")\n",
    "#RddDataBase.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkNodesFromTo(NodesRow):\n",
    "    nodeslinked = re.split(' ', NodesRow)\n",
    "    return nodeslinked[0], nodeslinked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', <pyspark.resultiterable.ResultIterable object at 0x0000016C9E69DA60>), ('11342', <pyspark.resultiterable.ResultIterable object at 0x0000016C9E69DB20>), ('824020', <pyspark.resultiterable.ResultIterable object at 0x0000016C9E69DB80>)]\n",
      "3\n",
      "[('0', 0.3333333333333333), ('11342', 0.3333333333333333), ('824020', 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "# First step :\n",
    "# Create key/value pairs, \n",
    "# where the key is the name of the page and the value is out-links from the page (Di) \n",
    "# and İnitiate PageRank values (Ri) as 1/Number of pages.\n",
    "#links = sc.textFile(\"web-GoogleSmall.txt\")\n",
    "\n",
    "# Key/value pairs\n",
    "links = RddDataBase.map(lambda NodesRow: linkNodesFromTo(NodesRow)).distinct().groupByKey().cache()\n",
    "#links = links.map(lambda x: (x.split(' ')[0], x.split(' ')[1:]))\n",
    "print(links.collect())\n",
    "\n",
    "# Find node count\n",
    "N = links.count()\n",
    "print(N)\n",
    "\n",
    "# Create and initialize the ranks\n",
    "ranks = links.map(lambda node: (node[0],1.0/N))\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.19047619047619047), ('11342', 0.08333333333333333), ('27469', 0.023809523809523808), ('309564', 0.023809523809523808), ('322178', 0.023809523809523808), ('38716', 0.023809523809523808), ('387543', 0.023809523809523808), ('427436', 0.023809523809523808), ('538214', 0.023809523809523808), ('638706', 0.023809523809523808), ('645018', 0.023809523809523808), ('824020', 0.08333333333333333), ('835220', 0.023809523809523808), ('856657', 0.023809523809523808), ('867923', 0.10714285714285714), ('891835', 0.10714285714285714), ('91807', 0.16666666666666666)]\n",
      "[('0', 0.047619047619047616), ('11342', 0.047619047619047616), ('27469', 0.005952380952380952), ('309564', 0.005952380952380952), ('322178', 0.005952380952380952), ('38716', 0.005952380952380952), ('387543', 0.005952380952380952), ('427436', 0.005952380952380952), ('538214', 0.005952380952380952), ('638706', 0.005952380952380952), ('645018', 0.005952380952380952), ('824020', 0.047619047619047616), ('835220', 0.005952380952380952), ('856657', 0.005952380952380952), ('867923', 0.05357142857142857), ('891835', 0.05357142857142857), ('91807', 0.041666666666666664)]\n",
      "[('0', 0.027210884353741496), ('11342', 0.011904761904761904), ('27469', 0.003401360544217687), ('309564', 0.003401360544217687), ('322178', 0.003401360544217687), ('38716', 0.003401360544217687), ('387543', 0.003401360544217687), ('427436', 0.003401360544217687), ('538214', 0.003401360544217687), ('638706', 0.003401360544217687), ('645018', 0.003401360544217687), ('824020', 0.011904761904761904), ('835220', 0.003401360544217687), ('856657', 0.003401360544217687), ('867923', 0.015306122448979591), ('891835', 0.015306122448979591), ('91807', 0.023809523809523808)]\n",
      "[('0', 0.006802721088435374), ('11342', 0.006802721088435374), ('27469', 0.0008503401360544217), ('309564', 0.0008503401360544217), ('322178', 0.0008503401360544217), ('38716', 0.0008503401360544217), ('387543', 0.0008503401360544217), ('427436', 0.0008503401360544217), ('538214', 0.0008503401360544217), ('638706', 0.0008503401360544217), ('645018', 0.0008503401360544217), ('824020', 0.006802721088435374), ('835220', 0.0008503401360544217), ('856657', 0.0008503401360544217), ('867923', 0.007653061224489796), ('891835', 0.007653061224489796), ('91807', 0.005952380952380952)]\n",
      "[('0', 0.003887269193391642), ('11342', 0.0017006802721088435), ('27469', 0.00048590864917395527), ('309564', 0.00048590864917395527), ('322178', 0.00048590864917395527), ('38716', 0.00048590864917395527), ('387543', 0.00048590864917395527), ('427436', 0.00048590864917395527), ('538214', 0.00048590864917395527), ('638706', 0.00048590864917395527), ('645018', 0.00048590864917395527), ('824020', 0.0017006802721088435), ('835220', 0.00048590864917395527), ('856657', 0.00048590864917395527), ('867923', 0.002186588921282799), ('891835', 0.002186588921282799), ('91807', 0.003401360544217687)]\n"
     ]
    }
   ],
   "source": [
    "#Map: For each node i, \n",
    "#calculate vote (Ri/Di) for each out-link of i and propagate to adjacent nodes.\n",
    "#Reduce: For each node i, sum the upcoming votes and update Rank value (Ri).\n",
    "#Repeat this Map-Reduce step until Rank values converge (stable or within a margin).\n",
    "nbIter=5\n",
    "for i in range(nbIter):\n",
    "    # Join graph info with rank info and propogate to all neighbors rank scores (rank/(number of neighbors)\n",
    "    # And add up ranks from all in-coming edges\n",
    "    ranks = links.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]])\\\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    print(ranks.sortByKey().collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

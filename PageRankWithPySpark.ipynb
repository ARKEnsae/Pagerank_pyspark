{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pendant les test pour stopper un SparkContext\n",
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connexion au cluster / test en local\n",
    "sc = SparkContext(\"local\", \"Page Rank With PySpark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "# Préparation du fichier avant utilisation dans l'algo\n",
    "\n",
    "# Taille du fichier initial = dfFile.shape = (5105039, 2)\n",
    "#dfFile = pd.read_csv(\"web-Google.txt\", sep='\\t', header=3)\n",
    "\n",
    "# Pour les tests récupérations des x premières lignes uniquement \n",
    "dfFile = pd.read_csv(\"data/web-Google.txt\", sep='\\t', header=3, nrows = 100)\n",
    "print(dfFile.shape)\n",
    "#Etant donné que nous ne sélectionons qu'une partie du fichier les noeuds sont filtrés en conséquence\n",
    "nodesList = dfFile['# FromNodeId'].unique()\n",
    "dfFileUniqueNodes = dfFile[dfFile['ToNodeId'].isin(nodesList)]\n",
    "#dfFile.to_csv(\"web-GoogleSmall.txt\", sep='\\t', header=False, index=False)\n",
    "dfFileUniqueNodes.to_csv(\"data/web-GoogleSmall.txt\", sep=' ', header=False, index=False)\n",
    "print(dfFileUniqueNodes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file \"GoogleReduc.txt\" from a local file system (available on all nodes),\n",
    "# and return it as an RDD of Strings.\n",
    "RddDataBase = sc.textFile(\"data/web-GoogleSmall.txt\")\n",
    "#RddDataBase.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkNodesFromTo(NodesRow):\n",
    "    nodeslinked = NodesRow.split(' ') \n",
    "    return nodeslinked[0], nodeslinked[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', <pyspark.resultiterable.ResultIterable object at 0x0000020B6D910520>), ('11342', <pyspark.resultiterable.ResultIterable object at 0x0000020B6B417610>), ('824020', <pyspark.resultiterable.ResultIterable object at 0x0000020B6D9014C0>), ('867923', <pyspark.resultiterable.ResultIterable object at 0x0000020B6CA8B5E0>), ('891835', <pyspark.resultiterable.ResultIterable object at 0x0000020B6DAA10D0>), ('1', <pyspark.resultiterable.ResultIterable object at 0x0000020B6DAA1160>), ('203402', <pyspark.resultiterable.ResultIterable object at 0x0000020B6DAA11C0>)]\n",
      "7\n",
      "[('0', 0.14285714285714285), ('11342', 0.14285714285714285), ('824020', 0.14285714285714285), ('867923', 0.14285714285714285), ('891835', 0.14285714285714285), ('1', 0.14285714285714285), ('203402', 0.14285714285714285)]\n"
     ]
    }
   ],
   "source": [
    "# First step :\n",
    "# Create key/value pairs, \n",
    "# where the key is the name of the page and the value is out-links from the page (Di) \n",
    "# and İnitiate PageRank values (Ri) as 1/Number of pages.\n",
    "\n",
    "# Key/value pairs\n",
    "links = RddDataBase.map(lambda NodesRow: linkNodesFromTo(NodesRow)).distinct().groupByKey().cache()\n",
    "print(links.collect())\n",
    "\n",
    "# Find node count\n",
    "N = links.count()\n",
    "print(N)\n",
    "\n",
    "# Create and initialize the ranks\n",
    "ranks = links.map(lambda node: (node[0],1.0/N))\n",
    "print(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.19047619047619047), ('1', 0.07142857142857142), ('11342', 0.13095238095238093), ('203402', 0.07142857142857142), ('223236', 0.14285714285714285), ('824020', 0.03571428571428571), ('867923', 0.17857142857142855), ('891835', 0.17857142857142855)]\n",
      "[('0', 0.1746031746031746), ('1', 0.03571428571428571), ('11342', 0.16666666666666663), ('203402', 0.03571428571428571), ('223236', 0.07142857142857142), ('824020', 0.047619047619047616), ('867923', 0.16269841269841268), ('891835', 0.16269841269841268)]\n",
      "[('0', 0.17989417989417986), ('1', 0.017857142857142856), ('11342', 0.1521164021164021), ('203402', 0.017857142857142856), ('223236', 0.03571428571428571), ('824020', 0.04365079365079365), ('867923', 0.1693121693121693), ('891835', 0.1693121693121693)]\n",
      "[('0', 0.1781305114638448), ('1', 0.008928571428571428), ('11342', 0.15784832451499117), ('203402', 0.008928571428571428), ('223236', 0.017857142857142856), ('824020', 0.044973544973544964), ('867923', 0.16666666666666666), ('891835', 0.16666666666666666)]\n",
      "[('0', 0.17871840094062316), ('1', 0.004464285714285714), ('11342', 0.1556437389770723), ('203402', 0.004464285714285714), ('223236', 0.008928571428571428), ('824020', 0.0445326278659612), ('867923', 0.1676954732510288), ('891835', 0.1676954732510288)]\n"
     ]
    }
   ],
   "source": [
    "#Map: For each node i, \n",
    "#calculate vote (Ri/Di) for each out-link of i and propagate to adjacent nodes.\n",
    "#Reduce: For each node i, sum the upcoming votes and update Rank value (Ri).\n",
    "#Repeat this Map-Reduce step until Rank values converge (stable or within a margin).\n",
    "nbIter=5\n",
    "for i in range(nbIter):\n",
    "    # Join graph info with rank info and propogate to all neighbors rank scores (rank/(number of neighbors)\n",
    "    # And add up ranks from all in-coming edges\n",
    "    ranks = links.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]])\\\n",
    "    .reduceByKey(lambda x,y: x+y)\n",
    "    print(ranks.sortByKey().collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
